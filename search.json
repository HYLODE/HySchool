[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HySchool",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "tutorials/bmcp_ch01.html",
    "href": "tutorials/bmcp_ch01.html",
    "title": "HySchool",
    "section": "",
    "text": "Chapter 1 notes\nSteve Harris\n2022-09-04\n\nBook code available on github\nNote re python libraries patsy is no longer actively developed and formulaic is recommended instead\n\nfrom numpy import asscalar, ndarray\n\n<class 'ImportError'>: cannot import name 'asscalar' from 'numpy' (/opt/homebrew/Caskroom/miniforge/base/envs/hyschool/lib/python3.9/site-packages/numpy/__init__.py)\n\n\n\n%matplotlib inline\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm\nfrom scipy import stats\nfrom scipy.stats import entropy\nfrom scipy.optimize import minimize\n\nWARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n\n\n<class 'ImportError'>: cannot import name 'asscalar' from 'numpy' (/opt/homebrew/Caskroom/miniforge/base/envs/hyschool/lib/python3.9/site-packages/numpy/__init__.py)\n\n\n\naz.style.use(\"arviz-grayscale\")\nplt.rcParams['figure.dpi'] = 300\nnp.random.seed(521)\nviridish = [(0.2823529411764706, 0.11372549019607843, 0.43529411764705883, 1.0),\n            (0.1450980392156863, 0.6705882352941176, 0.5098039215686274, 1.0),\n            (0.6901960784313725, 0.8666666666666667, 0.1843137254901961"
  },
  {
    "objectID": "tutorials/tb_ch02.html",
    "href": "tutorials/tb_ch02.html",
    "title": "HySchool",
    "section": "",
    "text": "import pandas as pd\n\n\ndef update(table):\n    \"\"\"Compute the posterior probabilities.\n    Table index = labels for each level of prior\n    Table columns = [prior, likelihood]\n    Table rows = prior values\n    \"\"\"\n    table['unnorm'] = table['prior'] * table['likelihood']\n    # Generate a 'normalising constant' to return to the probability scale\n    prob_data = table['unnorm'].sum()\n    table['posterior'] = table['unnorm'] / prob_data\n    return prob_data\n\nExercise: Suppose you have two coins in a box. One is a normal coin with heads on one side and tails on the other, and one is a trick coin with heads on both sides. You choose a coin at random and see that one of the sides is heads. What is the probability that you chose the trick coin?\nNotes:\n\nyou observed 1 head\nPr(head|trick)=1\nPr(head|normal)=0.5\n\nYou wish to know Pr(trick|head). So - Pr(trick) = 0.5 - Pr(normal) = 0.5\nThe likelihood is the prob of the data from the model. Prob of head given trick is 1\n\nThen by Bayes = prior * likelihood / total\nprior (0.5) * likelihood (1) / ((0.5 * 1) + (0.5 * 0.5)) where the denominator sums all the ways the data could arise for all possible values of the prior.\n0.5 / (0.5 + 0.25) = 2/3\n\n\n# Solution\n\ntable4 = pd.DataFrame(index=['Normal', 'Trick'])\ntable4['prior'] = 1/2\ntable4['likelihood'] = 1/2, 1\n\nupdate(table4)\ntable4\n\n\n\n\n\n  \n    \n      \n      prior\n      likelihood\n      unnorm\n      posterior\n    \n  \n  \n    \n      Normal\n      0.5\n      0.5\n      0.25\n      0.333333\n    \n    \n      Trick\n      0.5\n      1.0\n      0.50\n      0.666667"
  },
  {
    "objectID": "tutorials/tb_ch01.html",
    "href": "tutorials/tb_ch01.html",
    "title": "HySchool",
    "section": "",
    "text": "import pandas as pd\n\ngss = pd.read_csv('gss_bayes.csv', index_col=0)"
  },
  {
    "objectID": "tutorials/tb_ch01.html#political-views-and-parties",
    "href": "tutorials/tb_ch01.html#political-views-and-parties",
    "title": "HySchool",
    "section": "Political Views and Parties",
    "text": "Political Views and Parties\nThe other variables we’ll consider are polviews, which describes the political views of the respondents, and partyid, which describes their affiliation with a political party.\nThe values of polviews are on a seven-point scale:\n1   Extremely liberal\n2   Liberal\n3   Slightly liberal\n4   Moderate\n5   Slightly conservative\n6   Conservative\n7   Extremely conservative\nI’ll define liberal to be True for anyone whose response is “Extremely liberal”, “Liberal”, or “Slightly liberal”.\nThe values of partyid are encoded like this:\n0   Strong democrat\n1   Not strong democrat\n2   Independent, near democrat\n3   Independent\n4   Independent, near republican\n5   Not strong republican\n6   Strong republican\n7   Other party\nI’ll define democrat to include respondents who chose “Strong democrat” or “Not strong democrat”:\n\nliberal = (gss['polviews'] <= 3)\ndemocrat = (gss['partyid'] <= 1)\n\n\nfemale = gss['sex']==2 # 1=male, 2=female\nfemale.value_counts()\n\nTrue     26511\nFalse    22779\nName: sex, dtype: int64\n\n\n\nbanker = (gss['indus10'] == 6870)\nbanker.value_counts()\n\nFalse    48562\nTrue       728\nName: indus10, dtype: int64\n\n\n\ndef prob(A):\n    \"\"\"Computes the probability of a proposition, A.\"\"\"    \n    return A.mean()\n\n\ndef conditional(proposition, given):\n    \"\"\"Probability of A conditioned on given.\"\"\"\n    return prob(proposition[given])"
  },
  {
    "objectID": "tutorials/tb_ch01.html#exercises",
    "href": "tutorials/tb_ch01.html#exercises",
    "title": "HySchool",
    "section": "Exercises",
    "text": "Exercises\nExercise: Let’s use the tools in this chapter to solve a variation of the Linda problem.\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1. Linda is a banker. 2. Linda is a banker and considers herself a liberal Democrat.\n\nTo answer this question, compute\n\nThe probability that Linda is a female banker,\nThe probability that Linda is a liberal female banker, and\nThe probability that Linda is a liberal female banker and a Democrat.\n\n\n# Solution goes here\n\n\n# then logical 'and' the two series and return the 'mean' to get the proportion/fraction/prob of the conjunction\n(female & banker).mean()\n\n0.011381618989653074\n\n\n\n# Solution goes here\n(liberal & female & banker).mean()\n\n0.002556299452221546\n\n\n\n# Solution goes here\n(liberal & female & banker & democrat).mean()\n\n0.0012375735443294787\n\n\n\n# Solution goes here\n\nExercise: Use conditional to compute the following probabilities:\n\nWhat is the probability that a respondent is liberal, given that they are a Democrat?\nWhat is the probability that a respondent is a Democrat, given that they are liberal?\n\nThink carefully about the order of the arguments you pass to conditional.\n\n# Solution goes here\nlen(gss[liberal & democrat]) / len(gss[democrat])\n\n0.3891320002215698\n\n\n\nliberal[democrat].mean()\n\n0.3891320002215698\n\n\n\n# Solution goes here\n\nExercise: There’s a famous quote about young people, old people, liberals, and conservatives that goes something like:\n\nIf you are not a liberal at 25, you have no heart. If you are not a conservative at 35, you have no brain.\n\nWhether you agree with this proposition or not, it suggests some probabilities we can compute as an exercise. Rather than use the specific ages 25 and 35, let’s define young and old as under 30 or over 65:\n\nyoung = (gss['age'] < 30)\nprob(young)\n\n0.19435991073240008\n\n\n\nold = (gss['age'] >= 65)\nprob(old)\n\n0.17328058429701765\n\n\nFor these thresholds, I chose round numbers near the 20th and 80th percentiles. Depending on your age, you may or may not agree with these definitions of “young” and “old”.\nI’ll define conservative as someone whose political views are “Conservative”, “Slightly Conservative”, or “Extremely Conservative”.\n\nconservative = (gss['polviews'] >= 5)\nprob(conservative)\n\n0.3419354838709677\n\n\nUse prob and conditional to compute the following probabilities.\n\nWhat is the probability that a randomly chosen respondent is a young liberal?\nWhat is the probability that a young person is liberal?\nWhat fraction of respondents are old conservatives?\nWhat fraction of conservatives are old?\n\nFor each statement, think about whether it is expressing a conjunction, a conditional probability, or both.\nFor the conditional probabilities, be careful about the order of the arguments. If your answer to the last question is greater than 30%, you have it backwards!\n\n# Solution goes here\nprob(young & liberal)\n\n0.06579427875836884\n\n\n\n# Solution goes here\nyoung[liberal].mean()\n\n0.24034684651300675\n\n\n\n# Solution goes here\nprob(old & conservative)\n\n0.06701156421180766\n\n\n\n# Solution goes here\nprob(old & conservative) / prob(conservative)\n\n0.19597721609113564"
  },
  {
    "objectID": "tutorials/chap01_ex.html",
    "href": "tutorials/chap01_ex.html",
    "title": "HySchool",
    "section": "",
    "text": "import pandas as pd\n\ngss = pd.read_csv('gss_bayes.csv', index_col=0)"
  },
  {
    "objectID": "tutorials/chap01_ex.html#political-views-and-parties",
    "href": "tutorials/chap01_ex.html#political-views-and-parties",
    "title": "HySchool",
    "section": "Political Views and Parties",
    "text": "Political Views and Parties\nThe other variables we’ll consider are polviews, which describes the political views of the respondents, and partyid, which describes their affiliation with a political party.\nThe values of polviews are on a seven-point scale:\n1   Extremely liberal\n2   Liberal\n3   Slightly liberal\n4   Moderate\n5   Slightly conservative\n6   Conservative\n7   Extremely conservative\nI’ll define liberal to be True for anyone whose response is “Extremely liberal”, “Liberal”, or “Slightly liberal”.\nThe values of partyid are encoded like this:\n0   Strong democrat\n1   Not strong democrat\n2   Independent, near democrat\n3   Independent\n4   Independent, near republican\n5   Not strong republican\n6   Strong republican\n7   Other party\nI’ll define democrat to include respondents who chose “Strong democrat” or “Not strong democrat”:\n\nliberal = (gss['polviews'] <= 3)\ndemocrat = (gss['partyid'] <= 1)\n\n\nfemale = gss['sex']==2 # 1=male, 2=female\nfemale.value_counts()\n\nTrue     26511\nFalse    22779\nName: sex, dtype: int64\n\n\n\nbanker = (gss['indus10'] == 6870)\nbanker.value_counts()\n\nFalse    48562\nTrue       728\nName: indus10, dtype: int64\n\n\n\ndef prob(A):\n    \"\"\"Computes the probability of a proposition, A.\"\"\"    \n    return A.mean()\n\n\ndef conditional(proposition, given):\n    \"\"\"Probability of A conditioned on given.\"\"\"\n    return prob(proposition[given])"
  },
  {
    "objectID": "tutorials/chap01_ex.html#exercises",
    "href": "tutorials/chap01_ex.html#exercises",
    "title": "HySchool",
    "section": "Exercises",
    "text": "Exercises\nExercise: Let’s use the tools in this chapter to solve a variation of the Linda problem.\n\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1. Linda is a banker. 2. Linda is a banker and considers herself a liberal Democrat.\n\nTo answer this question, compute\n\nThe probability that Linda is a female banker,\nThe probability that Linda is a liberal female banker, and\nThe probability that Linda is a liberal female banker and a Democrat.\n\n\n# Solution goes here\n\n\n# then logical 'and' the two series and return the 'mean' to get the proportion/fraction/prob of the conjunction\n(female & banker).mean()\n\n0.011381618989653074\n\n\n\n# Solution goes here\n(liberal & female & banker).mean()\n\n0.002556299452221546\n\n\n\n# Solution goes here\n(liberal & female & banker & democrat).mean()\n\n0.0012375735443294787\n\n\n\n# Solution goes here\n\nExercise: Use conditional to compute the following probabilities:\n\nWhat is the probability that a respondent is liberal, given that they are a Democrat?\nWhat is the probability that a respondent is a Democrat, given that they are liberal?\n\nThink carefully about the order of the arguments you pass to conditional.\n\n# Solution goes here\nlen(gss[liberal & democrat]) / len(gss[democrat])\n\n0.3891320002215698\n\n\n\nliberal[democrat].mean()\n\n0.3891320002215698\n\n\n\n# Solution goes here\n\nExercise: There’s a famous quote about young people, old people, liberals, and conservatives that goes something like:\n\nIf you are not a liberal at 25, you have no heart. If you are not a conservative at 35, you have no brain.\n\nWhether you agree with this proposition or not, it suggests some probabilities we can compute as an exercise. Rather than use the specific ages 25 and 35, let’s define young and old as under 30 or over 65:\n\nyoung = (gss['age'] < 30)\nprob(young)\n\n0.19435991073240008\n\n\n\nold = (gss['age'] >= 65)\nprob(old)\n\n0.17328058429701765\n\n\nFor these thresholds, I chose round numbers near the 20th and 80th percentiles. Depending on your age, you may or may not agree with these definitions of “young” and “old”.\nI’ll define conservative as someone whose political views are “Conservative”, “Slightly Conservative”, or “Extremely Conservative”.\n\nconservative = (gss['polviews'] >= 5)\nprob(conservative)\n\n0.3419354838709677\n\n\nUse prob and conditional to compute the following probabilities.\n\nWhat is the probability that a randomly chosen respondent is a young liberal?\nWhat is the probability that a young person is liberal?\nWhat fraction of respondents are old conservatives?\nWhat fraction of conservatives are old?\n\nFor each statement, think about whether it is expressing a conjunction, a conditional probability, or both.\nFor the conditional probabilities, be careful about the order of the arguments. If your answer to the last question is greater than 30%, you have it backwards!\n\n# Solution goes here\nprob(young & liberal)\n\n0.06579427875836884\n\n\n\n# Solution goes here\nyoung[liberal].mean()\n\n0.24034684651300675\n\n\n\n# Solution goes here\nprob(old & conservative)\n\n0.06701156421180766\n\n\n\n# Solution goes here\nprob(old & conservative) / prob(conservative)\n\n0.19597721609113564"
  },
  {
    "objectID": "tutorials/think-bayes.html",
    "href": "tutorials/think-bayes.html",
    "title": "Think Bayes",
    "section": "",
    "text": "and some bayes stuff"
  },
  {
    "objectID": "tutorials/tutorials.html",
    "href": "tutorials/tutorials.html",
    "title": "Some tutorials",
    "section": "",
    "text": "Now you can learn something"
  },
  {
    "objectID": "onboarding/safe-data-tips.html",
    "href": "onboarding/safe-data-tips.html",
    "title": "HySchool",
    "section": "",
    "text": "The End\nA template JupyterNotebook for working with data at UCLH. The following features of this notebook, and associated files are documented here to minimise the risk of data leaks or other incidents.\n\nUsernames and passwords are stored in a .env file that is excluded from version control. The example env file at ./config/env should be edited and saved as ./config/.env. A utility function load_env_vars() is provided that will confirm this file exists and load the configuration into the working environment.\n.gitattributes are set to strip JupyterNotebook cells when pushing to GitHub"
  },
  {
    "objectID": "emap/uclh-working-with-star.html",
    "href": "emap/uclh-working-with-star.html",
    "title": "HySchool",
    "section": "",
    "text": "A template JupyterNotebook for working with EMAP. The following features of this notebook, and associated files are documented here to minimise the risk of data leaks or other incidents.\n\nUsernames and passwords are stored in a .env file that is excluded from version control. The example env file at ./config/env should be edited and saved as ./config/.env. A utility function load_env_vars() is provided that will confirm this file exists and load the configuration into the working environment.\n.gitattributes are set to strip JupyterNotebook cells when pushing to GitHub\n\n\n\nLoad libraries\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sqlalchemy import create_engine\n\n\nfrom utils.setup import load_env_vars\n\n\n\n\nLoad environment variables and set-up SQLAlchemy connection engine for the EMAP Star\n\n# Load environment variables\nload_env_vars()\n\n# Construct the PostgreSQL connection\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\nThe above code is also abstracted into a function (below) but shown in long form above to make clear what we are doing.\nfrom utils.setup import make_emap_engine\nemapdb_engine = make_emap_engine\n\n\n\nNow use the connection to work with EMAP.\nFor example, let’s inspect patients currently in ED or Resus.\nHere’s the SQL:\n-- Example script \n-- to pick out patients currently in A&E resus or majors\n\nSELECT\n   vd.location_visit_id\n  ,vd.hospital_visit_id\n  ,vd.location_id\n  -- ugly HL7 location string \n  ,lo.location_string\n  -- time admitted to that bed/theatre/scan etc.\n  ,vd.admission_time\n  -- time discharged from that bed\n  ,vd.discharge_time\n\nFROM star.location_visit vd\n-- location label\nINNER JOIN star.location lo ON vd.location_id = lo.location_id\nWHERE \n-- last few hours\nvd.admission_time > NOW() - '12 HOURS'::INTERVAL    \n-- just CURRENT patients\nAND\nvd.discharge_time IS NULL\n-- filter out just ED and Resus or Majors\nAND\n-- unpacking the HL7 string formatted as \n-- Department^Ward^Bed string\nSPLIT_PART(lo.location_string,'^',1) = 'ED'\nAND\nSPLIT_PART(lo.location_string,'^',2) ~ '(RESUS|MAJORS)'\n-- sort\nORDER BY lo.location_string\n;\nThe SQL script is stored at ../snippets/sql-vignettes/current_bed.sql.\nWe can load the script, and read the results into a Pandas dataframe.\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/current_bed.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\n\ndf.head()\n\n\n\n\nA series of three scripts\n\nSimply pull hospital visits\nAdd in hospital numbers (MRN) and handle patient merges\nAdd in patient demographics\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  -- admission to hospital\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  -- discharge from hospital\n  -- NB: Outpatients have admission events but not discharge events\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time > NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_1.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSee the series of joins in the middle of the script that retrieve the live MRN. That is we recognise that patients may have had an episode of care with one MRN, and then that episode was merged with another historical MRN. One of those two MRNs will then become the ‘live’ MRN and can be used to trace the patient across what otherwise would be different identities.\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n-- start from hospital visits\nFROM star.hospital_visit vo\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time > NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_2.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n  -- core demographics\n  ,cd.date_of_birth\n  -- convert dob to age in years\n  ,date_part('year', AGE(cd.date_of_birth)) AS age\n  ,cd.sex\n  ,cd.home_postcode\n  -- grab initials from first and last name\n  ,CONCAT(LEFT(cd.firstname, 1), LEFT(cd.lastname, 1)) AS initials\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nINNER JOIN star.core_demographic cd ON vo.mrn_id = cd.mrn_id\n\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time > NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_3.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\n\nWIP: example script to work with vitals\n-- Example script showing how to work with observations\n\n-- V simple view that finds recent observations \n-- for current inpatients in the last few minutes\n\n\nSELECT\n  -- observation details\n   ob.visit_observation_id\n  ,ob.hospital_visit_id\n  ,ob.observation_datetime\n\n  --,ob.visit_observation_type_id\n  --,ot.id_in_application\n\n  -- label nicely\n  ,CASE \n    WHEN ot.id_in_application = '10' THEN 'SpO2'\n    WHEN ot.id_in_application = '5' THEN 'BP'\n    WHEN ot.id_in_application = '3040109304' THEN 'Oxygen'\n    WHEN ot.id_in_application = '6' THEN 'Temp'\n    WHEN ot.id_in_application = '8' THEN 'Pulse'\n    WHEN ot.id_in_application = '9' THEN 'Resp'\n    WHEN ot.id_in_application = '6466' THEN 'AVPU'\n\n  END AS vital\n\n  ,ob.value_as_real\n  ,ob.value_as_text\n  ,ob.unit \n  \nFROM\n  star.visit_observation ob\n-- observation look-up\nLEFT JOIN\n  star.visit_observation_type ot\n  on ob.visit_observation_type_id = ot.visit_observation_type_id\n\nWHERE\nob.observation_datetime > NOW() - '5 MINS'::INTERVAL    \nAND\not.id_in_application in \n\n  (\n  '10'            --'SpO2'                  -- 602063230\n  ,'5'            --'BP'                    -- 602063234\n  ,'3040109304'   --'Room Air or Oxygen'    -- 602063268\n  ,'6'            --'Temp'                  -- 602063248\n  ,'8'            --'Pulse'                 -- 602063237\n  ,'9'            --'Resp'                  -- 602063257\n  ,'6466'         -- Level of consciousness\n)\nORDER BY ob.observation_datetime DESC\n;\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/vital_signs.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\nNow let’s drill down on just heart rate\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/heart_rate.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\nimport plotly.express as px\nfigx = px.histogram(df, \n                    x='value_as_real',\n                    title='Heart rate distribution at UCLH in the last 24 hours',\n                   labels={'value_as_real': 'Heart Rate'})\nfigx.show()\n\n\n# end of notebook"
  },
  {
    "objectID": "emap/how_to_use_this_guide.html",
    "href": "emap/how_to_use_this_guide.html",
    "title": "HySchool",
    "section": "",
    "text": "Read through the material on the website\nInteractive examples\n\nFork the repository https://github.com/hylode/hyschool.git\nthen clone or download the forked repo to a UCLH machine.\nthen start Jupyter at the top level of the directory\nthe notebooks should then just run\n\nThat’s it for now."
  },
  {
    "objectID": "emap/EMAP_quick_tour.html",
    "href": "emap/EMAP_quick_tour.html",
    "title": "HySchool",
    "section": "",
    "text": "EMAP concept"
  },
  {
    "objectID": "hylode/HyMind-ML-example.html",
    "href": "hylode/HyMind-ML-example.html",
    "title": "HySchool",
    "section": "",
    "text": "from typing import List\nimport os\nimport tempfile\nfrom pathlib import Path\nimport pickle\nfrom uuid import uuid4\n\nimport cloudpickle\nimport pandas as pd\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import ParameterGrid, train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom sklearn.pipeline import Pipeline as SKPipeline\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    StandardScaler,\n)\n\nfrom hycastle.lens.base import BaseLens\nfrom hycastle.lens.transformers import DateTimeExploder, timedelta_as_hours\n\n%matplotlib inline\n\n\nsecret_path = 'secret'\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path(secret_path).read_text().strip().split('\\n')\n\n\nfrom hylib.dt import LONDON_TZ\nfrom hycastle.lens.icu import BournvilleICUSitRepLens\nfrom hycastle.icu_store.live import live_dataset\nfrom hycastle.icu_store.retro import retro_dataset\nfrom hymind.lib.models.icu_aggregate import AggregateDemandModel\n\n\n\n\n\nmlflow_var = os.getenv('HYMIND_REPO_TRACKING_URI')\nmlflow.set_tracking_uri(mlflow_var)   \n\n\nclient = MlflowClient()\n\n\n\n\n\ndf = retro_dataset('T03')\n\n\ndf.shape\n\n\ndf.head()\n\n\n\n\n# lens = BournvilleICUSitRepLens()\n\n\nclass DemoLens(BaseLens):\n    numeric_output = True\n    index_col = \"episode_slice_id\"\n\n    @property\n    def input_cols(self) -> List[str]:\n        return [\n            \"episode_slice_id\",\n            \"admission_age_years\",\n            \"avg_heart_rate_1_24h\",\n            \"max_temp_1_12h\",\n            \"avg_resp_rate_1_24h\",\n            \"elapsed_los_td\",\n            \"admission_dt\",\n            \"horizon_dt\",\n            \"n_inotropes_1_4h\",\n            \"wim_1\",\n            \"bay_type\",\n            \"sex\",\n            \"vent_type_1_4h\",\n        ]\n\n    def specify(self) -> ColumnTransformer:\n        return ColumnTransformer(\n            [\n                (\n                    \"select\",\n                    \"passthrough\",\n                    [\n                        \"episode_slice_id\",\n                        \"admission_age_years\",\n                        \"n_inotropes_1_4h\",\n                        \"wim_1\",\n                    ],\n                ),\n                (\"bay_type_enc\", OneHotEncoder(), [\"bay_type\"]),\n                (\n                    \"sex_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"sex\"],\n                ),\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\", \"horizon_dt\"],\n                ),\n                (\n                    \"vent_type_1_4h_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"vent_type_1_4h\"],\n                ),\n                (\n                    \"vitals_impute\",\n                    SimpleImputer(strategy=\"mean\", add_indicator=False),\n                    [\n                        \"avg_heart_rate_1_24h\",\n                        \"max_temp_1_12h\",\n                        \"avg_resp_rate_1_24h\",\n                    ],\n                ),\n                (\n                    \"elapsed_los_td_hrs\",\n                    FunctionTransformer(timedelta_as_hours),\n                    [\"elapsed_los_td\"],\n                ),\n            ]\n        )\n\n\nlens = DemoLens()\n\n\nX = lens.fit_transform(df)\n\n\ny = df['discharged_in_48hr'].astype(int)\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n\n\n\n\n\nm = RandomForestClassifier(n_jobs=-1, n_estimators=50, max_depth=2)\n%time m.fit(X_train.values, y_train.values.ravel())\n\n\n\n\n\n\n\n\ntmp_path = Path('tmp')\ntmp_path.mkdir(parents=True, exist_ok=True)\n\ndef mlflow_log_string(text, filename):\n    full_path = tmp_path / filename\n    with open(full_path, 'w') as f:\n        f.write(str(text))\n    mlflow.log_artifact(full_path)\n\ndef mlflow_log_tag_dict(tag_dict, filename):\n    \"\"\"Logs tag dict to MLflow (while preserving order unlike mlflow.log_dict)\"\"\"\n    full_path = tmp_path / filename\n    with open(full_path, 'w') as f:\n        yaml.dump(tag_dict, f, sort_keys=False)\n    mlflow.log_artifact(full_path)\n    \ndef mlflow_log_lens(l):\n    full_path = l.pickle(tmp_path)\n    mlflow.log_artifact(full_path, 'lens')\n\n\n# Owner|Type|Name|Date\nexp_name = 'NS|models|jendemo|2021-10-05'\n\n\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"] = exp_name\nexperiment_id = mlflow.create_experiment(exp_name)\n\nexperiment_id\n\n\ndef artifact_path():\n    pth = Path(mlflow.get_artifact_uri())\n    pth.mkdir(parents=True, exist_ok=True)\n    return pth\n\n\n\n\n\ngrid = {\n    'n_estimators':[5, 10],\n    'max_depth':[2, 10]\n}\n\n\n\n\n\nruns_per_param_set = 2\n\nfor i in range(runs_per_param_set):\n    \n    for g in ParameterGrid(grid):\n        m = RandomForestClassifier(n_jobs=-1)\n\n        with mlflow.start_run():\n            #mlflow_logs()\n            \n            m.set_params(**g)\n            mlflow.log_params(g)\n\n            m.fit(X_train.values, y_train.values.ravel())\n            \n            eval_df = pd.DataFrame({\n                        'predict_proba':m.predict_proba(X_valid.values)[:,1], \n                        'label':y_valid.to_numpy().ravel()\n                       }, \n                columns=['predict_proba','label'])\n            \n            train_accuracy = m.score(X_train, y_train.to_numpy())\n            mlflow.log_metric('train_accuracy', train_accuracy)\n            valid_accuracy = m.score(X_valid, y_valid.to_numpy())       \n            mlflow.log_metric('valid_accuracy', valid_accuracy)\n            \n            train_confusion = confusion_matrix(m.predict(X_train.values), y_train.to_numpy())\n            mlflow_log_string(train_confusion, 'train_confusion.txt')\n            valid_confusion = confusion_matrix(m.predict(X_valid.values), y_valid.to_numpy())\n            mlflow_log_string(valid_confusion, 'valid_confusion.txt')\n\n            mlflow.sklearn.log_model(m, 'model')\n\n\n\n\n\nruns = mlflow.search_runs()\nruns.head()\n\n\nparams = [col for col in runs if col.startswith('params')]\nbest_params = runs.groupby(params)['metrics.valid_accuracy'].mean().idxmax()\nbest_row = runs.set_index(keys=params).loc[best_params]\n\nbest_run_id = list(best_row['run_id'])[0]\nbest_run_id\n\n\n\n\nwith mlflow.start_run(run_id=best_run_id):\n    # tag the run as best_row\n    mlflow.set_tag('best_run', 1)   \n\n\n\n\n\nwith mlflow.start_run(run_id=best_run_id):\n     mlflow_log_lens(lens)\n\n\n\n\n\n\nmodel_name = 'demo-model-jen'\nversion = 1\n\n\nmlflow.register_model(f'runs:/{best_run_id}/model', model_name)\n\n\n\n\n\n\n\n\n\nmodel_info = client.get_model_version(model_name, version)\nmodel_info\n\n\nrun_info = client.get_run(model_info.run_id)\nrun_info\n\n\n\n\n\nmodel = mlflow.sklearn.load_model(f'models:/{model_name}/{version}')\n\n\nmodel\n\n\n\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n    \n    client.download_artifacts(model_info.run_id, 'lens', tmp_dir)\n    \n    lens_path = next((tmp_dir / 'lens').rglob('*.pkl'))\n    with open(lens_path, 'rb') as f:\n        loaded_lens = pickle.load(f)\n\n\nloaded_lens\n\n\n\n\n\n\n\nlive_df = live_dataset('T03')\n\n\nlive_df.loc[:, ['episode_slice_id', 'admission_dt', 'bed_code', 'avg_heart_rate_1_24h']].sort_values('admission_dt', ascending=False).head()\n\n\nX_df = loaded_lens.transform(live_df)\n\n\npredictions = model.predict_proba(X_df)\n\n\nlive_df['prediction'] = predictions[:, 1]\n\nlive_df.loc[:, [‘episode_slice_id’, ‘prediction’]]"
  },
  {
    "objectID": "hylode/vignette_0__README.html",
    "href": "hylode/vignette_0__README.html",
    "title": "HySchool",
    "section": "",
    "text": "Case to the ML4H researcher…\nDoes all this come at the expense of ease and speed for the ML researcher?\nWe hope not. Engineered into Hylode are abstractions crafted to make the specific kind of modelling EHR ML practitioners do easier, faster and more joyful.\nIn particular:\n\nHylode provides a curated set of features easily interfacing w/ other UCLH data resources such as EMAP & Caboodle. Over time, we hope the store of pre-canned features will grow, cutting the time spent on wrangling, and freeing it up for more modelling work.\nBuilt for time-series EHR data Hylode automates many of the more fiddly elements of training a time-series ML4H model. It eases splitting patient records into temporal slices (the training instances needed to feed time-series EHR models).\nEase of experimental logging & model development The Hylode system makes it easy to log results & create reproducible workflows. The same tooling then makes it straightforward to pass models over from ML teams to application developers, to then present the predictions in a clinically meaningful way.\n\nWe see these three strengths as coming together to make Hylode a central resource to develop deployable ML models at UCLH. The core driver here is that Hylode allows researchers to get their modelling done faster.\nFrom there, the deployment benefits of Hylode kick in. Starting from a single trained model, Hylode offers a seamless technical transition to running models in silent mode, and a clear path to application development and testing the real-time relevance of predictions in complex multi-disciplinary clinical settings.\n\n\nThe notebooks…\nBackground over, we can now get started on the tutorial. The basic structure of these notebooks is as follows:\n\nNotebook 0 A compact end-to-end modelling exemplar evidencing the claims above in c. 20 lines of code.\n\nFrom there, we switch to a more measured pace. The aim is to give the reader enough grounding to start working with the different system components themselves:\n\nNotebook 1 (HyCastle) More detail on feature access and preprocessing.\nNotebook 2 (HyMind) A fuller end-to-end modelling platform exemplifying how we have been using the platform so far.\nNotebook 3 (Appendix - HyFlow & HyGear) For those interested, a delve under the hood. Here you can start triangulating yourself as to how the data is pulled from EMAP.\n\n\n1. Seneviratne, M. G., Shah, N. H. & Chu, L. Bridging the implementation gap of machine learning in healthcare. Bmj Innovations 6, 45 (2020)."
  },
  {
    "objectID": "hylode/vignette_0_intro.html",
    "href": "hylode/vignette_0_intro.html",
    "title": "HySchool",
    "section": "",
    "text": "Retrospective training set. Extraction & pre-processing\nFor the sake of these notebooks, we’re going to consider the problem of modelling ICU discharge at 48 hours.\n\n# First off, Hylode immediately gives us the features we need to train a time series model\nfrom hycastle.icu_store.retro import retro_dataset\n\ntrain_df = retro_dataset('T03')\ntrain_df.shape\n\nThis single piece of code has done a lot of work behind the scenes.\nIt has pulled data from EMAP, processed it where necessary to create features and then cut those features up so we have one row per hour for each patient – setting us up to make live predictions every hour for each patient on the unit.\nLet’s have a look at which features we have:\n\ntrain_df.columns\n\nThis is great, but we still have some categorical variables etc. laced in there. What happens if I want to do some pre-processing?\nThe answer lies in our lens abstraction, let’s have a look at one I prepared earlier:\n\nfrom hycastle.lens.icu import BournvilleICUSitRepLens\nlens = BournvilleICUSitRepLens()\n\nX_train = lens.fit_transform(train_df)\nX_train.columns\n\nWe can see we’ve just done some useful things. The lens’s fit_transform method has inserted missingness values for ethnicity, and we have broken out each patient’s admissions time into separate features as we think that will improve our model.\nWe also define a label:\n\ny_train = train_df['discharged_in_48hr'].astype(int)\n\n\n\nTraining & storing a model\nWith this “heavy” lifting done, we should now already be in a position to train a model. Let’s have a go:\n\nfrom sklearn.ensemble import RandomForestClassifier\nm = RandomForestClassifier(n_jobs=-1)\nm.fit(X_train.values, y_train.values.ravel())\n\nEverything seems to be working. Let’s imagine we’re happy with what we’ve done. Let’s log the model in our model repo, so it’s primed and ready to deploy…\n\nimport os\nimport mlflow\nmlflow_server = os.getenv('HYMIND_REPO_TRACKING_URI')\nmlflow.set_tracking_uri(mlflow_server)\n\n\nfrom datetime import datetime\nimport random\nimport string\n\n# Generate a unique experiment name\nexp_name = \"vignette_0-\" + \"\".join( random.sample(string.ascii_lowercase, k=8)) + str(datetime.now().timestamp())\n\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"] = exp_name\nexperiment_id = mlflow.create_experiment(exp_name)\n\nexperiment_id\n\n\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(m, 'model')\n\nThis screenshot shows what it looks like for the model to land safely in MLFlow (which you should be able to see for yourself if you follow the link here – look for a new experiment at the very bottom of the list on the left hand side)\n\n\nLoading and deploying a model\nNow let’s switch hats and imagine they were are trying to deploy the model in silent mode for the patients currently on the ICU. This is now pretty straightforward:\n\nfrom hycastle.icu_store.live import live_dataset\nlive_df = live_dataset('T03')\nlive_df.shape\n\n\nlive_df.columns\n\n\nX_df = lens.transform(live_df)\n\n\nruns = mlflow.search_runs(experiment_ids=[experiment_id])\nrun_id = runs.iloc[0].run_id\nrun_id\n\n\nlogged_model = f'runs:/{run_id}/model'\nloaded_model = mlflow.sklearn.load_model(logged_model)\n\n\npredictions = loaded_model.predict_proba(X_df.values)\nlive_df['prediction'] = predictions[:, 1]\nlive_df.loc[:, ['bed_code', 'prediction']].head()"
  },
  {
    "objectID": "hylode/vignette_1_training.html",
    "href": "hylode/vignette_1_training.html",
    "title": "HySchool",
    "section": "",
    "text": "HyCastle, the lens and building a training set\nIn the previous notebook, we showed an end-to-end exemplar of the Hylode platform - but we skated top speed over some details worth spending more time on.\nHere we take a more measured pace and zoom in on HyCastle and the lens. Together these two abstractions make it easy to ask Hylode for a retrospective training set and then to pre-process that training set.\nHylode does this in a way that allows the same underlying code to furnish the live data needed for deployable prediction.\n\n\nThe HyCastle module is the main workhorse for pulling the complete available feature set out of hylode_db (Hylode’s internal databases). Having defined our features in HyGear (covered in vignette 3), HyCastle can do two main things:\n~ it can pick out a training set comprising all the features for each hourly slice for each patient\n~ it can give us a live set of features for the patients currently on the ward\nLet’s try it out…\n\nfrom hycastle.icu_store.retro import retro_dataset\nfrom hycastle.icu_store.live import live_dataset # <-- includes PII\n\nward = 'T03'\n\n\n# the retro_dataset function gives us all the historical episode slices to build up our training set\ntrain_df = retro_dataset(ward)\ntrain_df.shape\n\n\n# and we can see the various feature columns we have generated\ntrain_df.head()\n\nThen using the same machinery, we can get the corresponding features for the patients currently on the ward.\nWhy this is important is that the same code is generating our training features and the features we will use to deploy the model (- ruling out unwanted surprises from divergence between the two!)\n\npredict_df = live_dataset(ward)\npredict_df.shape\n\n\npredict_df['horizon_dt'].head()\n\n\n\n\nIn the code above, we saw that HyCastle is very nifty in delivering us all the features we have pre-defined in hylode_db. But the question naturally arises, what if we want to use a subset of those features? Or to pre-process them in a specific way?\nWill this not require custom code - exposing us to the same risk of code divergence between training and deployment?\nOur answer to this is the lens. It is an abstraction that provides a more robust (transferrable) way to subset and pre-process the features coming out of HyCastle. Let’s have a look at a very simple example.\n\nfrom hycastle.lens.base import BaseLens\nfrom typing import List\nfrom sklearn.compose import ColumnTransformer\nfrom hycastle.lens.transformers import DateTimeExploder\n\n\nclass SimpleLens(BaseLens):\n    numeric_output = True\n    index_col = \"episode_slice_id\"\n\n    @property\n    def input_cols(self) -> List[str]:\n        return [\n            \"episode_slice_id\",\n            \"admission_dt\",\n        ]\n\n    def specify(self) -> ColumnTransformer:\n        return ColumnTransformer(\n            [\n                (\n                    \"select\",\n                    \"passthrough\",\n                    [\n                        \"episode_slice_id\"\n                    ],\n                ),\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\"],\n                ),\n            ]\n        )\n\nNotice that what we really have here is a list of 3-tuples to initialise the ColumnTransformer (which is a standard SKLearn class). For instance, the triple:\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\"],\n                )\nLet’s see what happens when we put this lens to work on the output from HyCastle\n\nlens = SimpleLens()\n\nX = lens.fit_transform(train_df)\nX.head()\n\n…basically we seem to have the episode_slice_id for every slice, and then a bunch of features about the admission_dt. In our original HyCastle dataset, we notice that admission_dt is a series of datetimes:\n\ntrain_df['admission_dt'].head()\n\n…but after we have transformed the retro dataframe, we have these additional admission features. This is thanks to the triple quoted above and the DateTimeExploder(). Let’s have a look to see what that code looks like…\n\n??DateTimeExploder\n\n\n??DateTimeExploder.transform\n\nIn short, what we are doing in defining a lens is defining a set of input columns from HyCastle that we want to work with, and then a sequence of column transformations (as a ColumnTransformer object) that we use to specifically define our pre-processing pathway.\nThis lens can then be used consistently between model training and deployment.\n\n\n\nHere’s a fuller and more complete example of a lens (along the lines of what we will use in the next vignette).\nIt might be worthwhile using the ?? shortcut to get a sense of the different transformations being applied.\n\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    StandardScaler,\n)\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom hycastle.lens.transformers import timedelta_as_hours\n\n\nclass DemoLens(BaseLens):\n    numeric_output = True\n    index_col = \"episode_slice_id\"\n\n    @property\n    def input_cols(self) -> List[str]:\n        return [\n            \"episode_slice_id\",\n            \"admission_age_years\",\n            \"avg_heart_rate_1_24h\",\n            \"max_temp_1_12h\",\n            \"avg_resp_rate_1_24h\",\n            \"elapsed_los_td\",\n            \"admission_dt\",\n            \"horizon_dt\",\n            \"n_inotropes_1_4h\",\n            \"wim_1\",\n            \"bay_type\",\n            \"sex\",\n            \"vent_type_1_4h\",\n        ]\n\n    def specify(self) -> ColumnTransformer:\n        return ColumnTransformer(\n            [\n                (\n                    \"select\",\n                    \"passthrough\",\n                    [\n                        \"episode_slice_id\",\n                        \"admission_age_years\",\n                        \"n_inotropes_1_4h\",\n                        \"wim_1\",\n                    ],\n                ),\n                (\"bay_type_enc\", OneHotEncoder(), [\"bay_type\"]),\n                (\n                    \"sex_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"sex\"],\n                ),\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\", \"horizon_dt\"],\n                ),\n                (\n                    \"vent_type_1_4h_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"vent_type_1_4h\"],\n                ),\n                (\n                    \"vitals_impute\",\n                    SimpleImputer(strategy=\"mean\", add_indicator=False),\n                    [\n                        \"avg_heart_rate_1_24h\",\n                        \"max_temp_1_12h\",\n                        \"avg_resp_rate_1_24h\",\n                    ],\n                ),\n                (\n                    \"elapsed_los_td_hrs\",\n                    FunctionTransformer(timedelta_as_hours),\n                    [\"elapsed_los_td\"],\n                ),\n            ]\n        )\n\n\nlens = DemoLens()\n\nX = lens.fit_transform(train_df)\nX.head()\n\n\nX.dtypes"
  },
  {
    "objectID": "hylode/vignette_3_data_flow.html",
    "href": "hylode/vignette_3_data_flow.html",
    "title": "HySchool",
    "section": "",
    "text": "What follows is a quick tour under the hood of Hylode…\nIn vignette_1_training_set, we miraculously wound up with a usable set of features on running retro_dataset. This is of course because they had been lovingly built by Nel in advance.\nThis notebook is aimed as a leg-up in getting your bearings around how Hylode ingests and processes data from EMAP. I include the different pieces of code and ways of thinking that have helped me, in the hope they will help others.\n\n\nAs good a starting point as any is the HySys architectural diagram linked to here. (You need to be logged into GitHub to view)\nThis picture gives an overview of how the system fits together. In terms of data ingestion, we can see HyFlow and HyGear, the two components responsible for fetching and transforming the data from EMAP (& other sources). Then sitting above them, there is HyCommand which controls different requests for the various subcomponents.\nIn the current version, an example of how HyCommand does its work can be found in the ICU Demand dag. This scheduled code triggers the appropriate actions from HyFlow and HyGear for initial ingestion and transformation of the data.\n(The PowerPoint slide DAG.pptx in this directory (download by opening in a new window) shows you the complete set operations the DAG triggers. Don’t be disheartened if this seems like a bit much, we will have a look at it piece-by-piece…)\n\n\n\nLooking at that file for the dag, let’s start by looking at the code here:\n        fetch_episode_slices_task = SimpleHttpOperator(\n            task_id=f\"fetch_episode_slices-{ward}\",\n            http_conn_id=\"hyflow_api\",\n            method=\"POST\",\n            endpoint=\"/trigger/fetch/icu/episode_slices\",\n            headers=default_http_headers,\n            data=json.dumps(fetch_task_input),\n            extra_options={\n                \"check_response\": False\n            },  # turn off the default response check\n            response_check=skip_on_empty_result_set,  # add a custom response check\n        )\nThis makes the API call to the hyflow_api to fetch the episode slices for a given ward. This can be found beautifully documented by looking at the HyFlow API docs (here at the time of writing). Here we can see that fetch_episode_slices is designed to: >Append Hylode episode slices to the hyflow.icu_episode_slices_log table for episodes which were active on the ward at the horizon.\n\nA Hylode episode is defined as a stay on a specific ward with a limited allowable break between bed location visits on that ward. An episode slice is a fraction, up to & incl 100%, of an episode.\n\n\n\nDigging a little bit deeper, we can trace this back to the SQL code. The code corresponding to fetch_episode_slices can be found in the function fetch_episode_slices found in the definition of the endpoint here. Here we can see the following code slice:\n    episode_slices_df = icu_episode_slices_from_emap(\n        ward, horizon_dt, list(beds_df.hl7_location)\n    )\nLet’s perhaps have a look at what this icu_episode_slices_from_emap function is…\n\nfrom hyflow.fetch.icu.icu_episode_slices import icu_episode_slices_from_emap\n\n\n??icu_episode_slices_from_emap\n\nOkay… so looking at this we can see that this function first call icu_location_visits_from_emap\n\nfrom hyflow.fetch.icu.icu_episode_slices import _icu_location_visits_from_emap\n\n\n??_icu_location_visits_from_emap\n\n…which in turn is running an sql query from file emap__icu_location_visit_history.sql. Looking this up in the Hylode code, we find the corresponding file here and can run the corresponding query in DBForge (being sure to substitute for the parameters prefixed by %)\nAlternatively we can do that here in a notebook… (see appendix 1)\n\n\n\nFollowing through on the rest of the definition of icu_episode_slices_from_emap, we can see this function goes onto call _coalesce_icu_location_visits_into_episode_slices which generates our notion of ICU location visits (as described in the functions docstring - see using the ?? shortcut).\nThen returning back again to the code for fetch_icu_episode_slices we can see a call df_to_hylode_db. This is where the dataframe extracted from EMAP and then restructured to episode slices is stored in the Hylode databases.\nA very comparable process happens to bring in the observations into Hylode, so with some ferreting out (along the lines above) it should be possible to find the corresponding pieces of code. Next up is to transform the data from there…\n\n\n\n\nIn talking about how the Hylode ML system works, often a lot of discussions come back to the transformers. These are the pieces of code that take the data from a format not a million miles from that in EMAP into reproducible features for both retrospective model training and deployment.\nAs in our section above on HyFlow the HyGear transformers are called on a schedule from the ICU Demand dag.\nTake for instance the code here:\n        generate_icu_temporal_task = SimpleHttpOperator(\n            task_id=f\"generate_icu_temporal-{ward}\",\n            http_conn_id=\"hygear_api\",\n            method=\"POST\",\n            endpoint=\"/trigger/generate/icu/cog1/temporal\",\n            headers=default_http_headers,\n            data=json.dumps(transform_task_input),\n        )\nThis makes the API call to the hygear_api to generate the ICU patient temporal features (age, elapsed length-of-stay etc.) for a given ward. This can be found beautifully documented by looking at the HyGear API docs (here at the time of writing). Here we can see that temporal is designed to: >Append temporal features to the hygear.icu_temporal_log table for episode slices active on the ward at the horizon.\n\n\nAgain we can go back to the definition of the endpoint found in this case here where we have the function generate_icu_temporal. This code allows us to actually look under the hood of the transformer. What we can see happening is that this code is pulling out the icu_patients_from_hyflow and applying a series of Transformer functions to them, namely: AdmissionAgeTransformer and LengthOfStayTransformer. Let’s have a look at one of these…\n\nfrom hygear.transform.cog1.icu_temporal import AdmissionAgeTransformer\n\n\nAdmissionAgeTransformer??\n\nWe can see that the transformer is a class. It takes a series of specified input_cols, and then has a defined transform method to output a specified set of output cols. Included in Appendix 2 is some code to run this transformer across a dataframe. You can use this same structure to develop and test new transformers in a notebook.\nOnce this feature transformation is done and dusted, we are ready to use Nels’ HyCastle machinery to pull together our feature sets for model training.\n\n\n\n\n(magpied from Nels’ existing HyMind exemplar)\n\nfrom datetime import datetime, timedelta\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\nimport urllib\n\nimport arrow\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom hylib.dt import LONDON_TZ, convert_dt_columns_to_london_tz\n\n\nward = 'T03'\n\n\n\nEMAP credentials are allocated per user and not stored in the environment variables. You do not want your credentials to leak into the source repository.\nOne way of safeguarding is to create a file called secret at the top level of the HyMind repository (one above this notebook).\nDo this here in Jupyter and not a local copy of the repo.\nThe first line should be your UDS username and the second line should be your UDS password.\nsecret has been added to .gitignore and will be excluded from the repository.\nRead your username & password into the environment:\n\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path('../secret').read_text().strip().split('\\n')\n\n\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nCreate a SQLAlchemy Engine for accessing the UDS:\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\n\nfrom hyflow.settings import SQL_DIR\nvisits_sql = (SQL_DIR / \"emap__icu_location_visit_history.sql\").read_text()\n\n\n# the point-in-time we are interested in:  7am on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 7, 0, 0).astimezone(LONDON_TZ)\n\n\nfrom hylib.load.hydef import beds_from_hydef\nbeds_df = beds_from_hydef(ward)\n\n\nvisits_df = pd.read_sql(\n    visits_sql,\n    emapdb_engine,\n    params={\"horizon_dt\": horizon_dt, \"locations\": list(beds_df.hl7_location)},\n)\n\n\nvisits_df.head()\n\n\n\n\n\n\nfrom datetime import datetime\nimport logging\n\nfrom fastapi import APIRouter\n\nfrom hylib.load.hydef import icu_observation_types_from_hydef\n\nfrom hyflow.load.icu.icu_episode_slices import icu_episode_slices_from_hyflow\nfrom hyflow.load.icu.icu_observations import icu_observations_from_hyflow\nfrom hyflow.load.icu.icu_patients import icu_patients_from_hyflow\n\nfrom hygear.transform.cog1.base import BaseCog1Transformer\nfrom typing import List\n\n\nclass AdmissionAgeTransformer(BaseCog1Transformer):\n    \"\"\"\n    An transformer for age at admission\n\n    Output Features:\n        `admission_age_years`: float\n            Patient's age in years\n    \"\"\"\n\n    input_cols = [\"episode_slice_id\", \"admission_dt\", \"dob\"]\n\n    @property\n    def output_cols(self) -> List[str]:\n        return [\"episode_slice_id\", \"admission_age_years\"]\n\n    def years(self, row: pd.Series) -> float:\n        if pd.isnull(row.dob):\n            return np.nan\n        else:\n            return int(row[\"admission_dt\"].year) - int(row[\"dob\"].year)\n\n    def transform(self) -> pd.DataFrame:\n        output_df = self.input_df\n\n        output_df[\"admission_age_years\"] = output_df.apply(self.years, axis=1)\n\n        return output_df.loc[:, self.output_cols]\n\n\nward\n\n\nhorizon_dt = datetime(2021, 10, 12, 11, 00).astimezone(LONDON_TZ)\n\n\nepisode_slices_df = icu_episode_slices_from_hyflow(ward, horizon_dt)\n\n\nepisode_slices_df.shape\n\n\npatients_df = icu_patients_from_hyflow(\n    ward, horizon_dt, list(episode_slices_df.episode_slice_id)\n)\n\n\nage_input_df = episode_slices_df.loc[:, [\"episode_slice_id\", \"admission_dt\"]].join(\n    patients_df.loc[:, [\"episode_slice_id\", \"dob\"]].set_index(\"episode_slice_id\"),\n    on=\"episode_slice_id\",\n)\n\n\nage_df = AdmissionAgeTransformer(ward, horizon_dt, age_input_df).transform()\noutput_df = episode_slices_df.loc[:, [\"episode_slice_id\"]].join(\n    age_df.set_index(\"episode_slice_id\"), on=\"episode_slice_id\"\n)\n\n\nage_df"
  },
  {
    "objectID": "hylode/vignette_2_modelling.html",
    "href": "hylode/vignette_2_modelling.html",
    "title": "HySchool",
    "section": "",
    "text": "In this notebook, we look at how the various different pieces of the Hylode architecture come together to ease the ML4H model development/deployment process.\nIn vignette_1_training_set, we looked at how HyCastle and the lens abstraction make for consistent training pathways between model development and deployment.\nHere, we bring these components together in a modelling workflow. Core steps are to show:\n~ how HyCastle and the lens come together to make our training & validation sets\n~ how we use MLFlow as a central spine for recording our model training\n~ how we then can check out models from MLFlow - either for further evaluation or live deployment"
  },
  {
    "objectID": "hylode/vignette_2_modelling.html#a-fuller-example",
    "href": "hylode/vignette_2_modelling.html#a-fuller-example",
    "title": "HySchool",
    "section": "A fuller example",
    "text": "A fuller example\nWith slightly better sense of how MLFlow works, we now turn to a fuller exemplar workflow. The example we look at here is running a simple parameter grid search for a Random Forest model of ICU discharge at 48 hours.\n\n# the two most influential parameters \n# cf. https://scikit-learn.org/stable/modules/ensemble.html#parameters\ngrid = {\n    'n_estimators':[10, 50, 100],\n    'max_features':[None, \"sqrt\", \"log2\"]\n}\n\n\n# as the outcome of each training run (even with the same parameters) is non-deterministic,\n# we run two training runs per parameter combination.\nruns_per_param_set = 2\n\nfor i in range(runs_per_param_set):\n    \n    for g in ParameterGrid(grid):\n        m = RandomForestClassifier(n_jobs=-1)\n\n        with mlflow.start_run():\n            \n            # logging the tag dictionary, the run_type\n            mlflow_log_tag_dict(tag_dict, 'tag_dict.yaml')\n            mlflow.set_tag(\"run_type\", \"training\")\n            \n            # set and log this run's set of model parameters\n            m.set_params(**g)\n            mlflow.log_params(g)\n\n            m.fit(X_train.values, y_train.values.ravel())\n            \n            # calculate and log training and validation set accuracy\n            train_accuracy = m.score(X_train.values, y_train.to_numpy())\n            mlflow.log_metric('train_accuracy', train_accuracy)\n            valid_accuracy = m.score(X_valid.values, y_valid.to_numpy())       \n            mlflow.log_metric('valid_accuracy', valid_accuracy)\n            \n            # ditto for confusion matrices\n            train_confusion = confusion_matrix(m.predict(X_train.values), y_train.to_numpy())\n            mlflow_log_string(train_confusion, 'train_confusion.txt')\n            valid_confusion = confusion_matrix(m.predict(X_valid.values), y_valid.to_numpy())\n            mlflow_log_string(valid_confusion, 'valid_confusion.txt')\n\n            # store the trained SKLearn model, so we can check it out later\n            mlflow.sklearn.log_model(m, 'model')\n\nAfter this cells runs (which takes a minute or two), if you now return to the MLFlow UI, you will see that the experiment you created is now populated with a whole list of runs, one for each parameter set above. Clicking down into the run you will see all the attributes above have been stored (the tag dictionary, the parameters, the metrics, the model etc.)\nAs a next step, we might want to pick out the model parameters that seem to have performed best - so we can then use these for further evaluation.\nThese runs can also be straightforwardly access from a notebook using mlflow.search_runs()…\n\nruns = mlflow.search_runs()\nruns.head()\n\nFrom which starting point, it’s simple to mark out the parameter set with the best mean validation accuracy, as follows:\n\nparams = [col for col in runs if col.startswith('params')]\nbest_params = runs.groupby(params)['metrics.valid_accuracy'].mean().idxmax()\nbest_row = runs.set_index(keys=params).loc[best_params]\n\nbest_run_id = list(best_row['run_id'])[0]\nbest_run_id\n\nAnd then we can tag this as the best run from our training loop - and also log the lens we used to train it:\n\nwith mlflow.start_run(run_id=best_run_id):\n    # tag the run as best_row\n    mlflow.set_tag('best_run', 1)   \n\n    # log the lens\n    mlflow_log_lens(lens)\n\nIn the same breath, MLFlow gives us the option to register our model, which makes it easy to access and work with going forward - so let’s do that too:\n\n# => add a unique model name below <=\n# e.g. tk-random_forest-demo\nmodel_name =\n\n\n# n.b. each time you run this cell with the same model_name, the model version will increase by one\nregistered_model = mlflow.register_model(f'runs:/{best_run_id}/model', model_name)\n\nWhich is great. And now you should be able to navigate to the MLFlow UI - and if you click on the ‘Models’ tab at the top of the page you should see your newly registered model waiting there on the list."
  },
  {
    "objectID": "hylode/vignette_2_modelling.html#forward-pass-prediction",
    "href": "hylode/vignette_2_modelling.html#forward-pass-prediction",
    "title": "HySchool",
    "section": "Forward pass prediction",
    "text": "Forward pass prediction\nWith the model and the lens loaded, the live_dataset from HyCastle makes it extremely straightforward to run the forward pass. (n.b. reusing the identical components to in our retrospective training)\n\nlive_df = live_dataset('T03')\nlive_df.shape\n\n\n# and inspecting the dataframe, note the most recent admission_dt\nlive_df.loc[:, ['episode_slice_id', 'admission_dt', 'bed_code', 'avg_heart_rate_1_24h']].sort_values('admission_dt', ascending=False).head()\n\nNow let’s try to run some patient-level predictions based on our saved model:\n\n# first we transform the live_df with our loaded_lens\nX_df = loaded_lens.transform(live_df)\nX_df.columns\n\n\n# making the predictions\npredictions = model.predict_proba(X_df.values)\n\n# adding the predictions to our live_df dataframe\nlive_df['prediction'] = predictions[:, 1]\nlive_df.loc[:, ['episode_slice_id', 'prediction']].head()\n\nWe can even then get a sense of how this segues into the aggregate prediction problem, using the AggregateDemandModel class:\n\nAggregateDemandModel??\n\n\nagg_demand = AggregateDemandModel()\nagg_predictions = agg_demand.predict(context=\"\", \n                                     model_input=live_df.loc[:, ['prediction']].rename(mapper={'prediction':'prediction_as_real'},axis=1))\nagg_predictions.plot()"
  },
  {
    "objectID": "hylode/vignette_2_modelling.html#further-evaluation",
    "href": "hylode/vignette_2_modelling.html#further-evaluation",
    "title": "HySchool",
    "section": "Further evaluation",
    "text": "Further evaluation\nAnother use case would be that having done our initial training, we still have plenty of work to do evaluating it’s performance. We give a very simple outline here of what that might look like.\nWe start by putting our two loaded components from MLFlow: loaded_tag_dict and loaded_lens together to rebuild our validation set.\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n    \n    client.download_artifacts(model_info.run_id, './', tmp_dir)\n    \n    tag_dict_path = tmp_dir / 'tag_dict.yaml'\n    with open(tag_dict_path, 'r') as stream:\n        loaded_tag_dict = yaml.load(stream, Loader=yaml.FullLoader)\n        \nloaded_tag_dict\n\n\nloaded_valid_df = df[(loaded_tag_dict['start_valid_dt'] < df['horizon_dt']) &\n                 (df['horizon_dt'] < loaded_tag_dict['end_valid_dt'])]\n\nRecreating our dataset as follows:\n\nX_valid = loaded_lens.transform(loaded_valid_df)\ny_valid = loaded_valid_df['discharged_in_48hr'].astype(int)\n\n\n# then we have already loaded in our model in the previous section\nmodel\n\n\nwith mlflow.start_run(run_id=best_run_id):\n    \n    mlflow_log_tag_dict(tag_dict, 'tag_dict.yaml')\n    \n    # create a 2-column dataframe of the predicted probabilities and true label,\n    # for each patient in the validation set\n    eval_df = pd.DataFrame({\n                'predict_proba':model.predict_proba(X_valid.values)[:,1], \n                'label':y_valid.to_numpy().ravel()\n               }, \n        columns=['predict_proba','label'],\n        index=X_valid.index)   \n    eval_df['horizon_dt'] = loaded_valid_df.set_index('episode_slice_id')['horizon_dt']\n    \n    # write eval_df to csv and log in MLFlow\n    eval_path = tmp_path / 'eval.csv'\n    eval_df.to_csv(eval_path)\n    mlflow.log_artifact(eval_path)\n    \n    \n    # use eval_df to store a new metric\n    eval_log_loss = log_loss(eval_df['label'],eval_df['predict_proba'])\n    mlflow.log_metric('log_loss', eval_log_loss)\n    \n    \n    # save a new figure alongside our registered model\n    eval_confusion = confusion_matrix(m.predict(X_valid.values), y_valid.to_numpy())\n    disp = ConfusionMatrixDisplay(confusion_matrix=eval_confusion,\n                              display_labels=['discharged','remained_after_48hrs'])\n    \n    confusion_path = tmp_path / 'confusion_fig_2.png'\n    disp.plot(cmap=plt.cm.Blues).figure_.savefig(confusion_path)\n    mlflow.log_artifact(confusion_path)"
  },
  {
    "objectID": "hylode/HyCastle-Lens.html",
    "href": "hylode/HyCastle-Lens.html",
    "title": "HySchool",
    "section": "",
    "text": "Please make a copy of this notebook and do not edit in place\nSee the system diagram for an overview of the HYLODE system components referenced in this notebook.\n(You will need to be signed into GitHub to view)\n\n\n\nfrom datetime import datetime, timedelta\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\nimport urllib\n\nimport arrow\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom hylib.dt import LONDON_TZ, convert_dt_columns_to_london_tz\nfrom hycastle.lens.icu import ICUSitRepUiLens, ICUSitRepLoSLens\n\n\n\n\n\nward = 'T03'\n\n\n\n\n\nAccess to EMAP is required for HyCastle to function properly.\n\n\nEMAP credentials are allocated per user and not stored in the environment variables. You do not want your credentials to leak into the source repository.\nOne way of safeguarding is to create a file called secret at the top level of the HyMind repository (next to this notebook).\nDo this here in Jupyter and not a local copy of the repo.\nThe first line should be your UDS username and the second line should be your UDS password.\nsecret has been added to .gitignore and will be excluded from the repository.\nRead your username & password into the environment:\n\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path('secret').read_text().strip().split('\\n')"
  },
  {
    "objectID": "hylode/HyCastle-Lens.html#retrospective-training-data",
    "href": "hylode/HyCastle-Lens.html#retrospective-training-data",
    "title": "HySchool",
    "section": "Retrospective Training Data",
    "text": "Retrospective Training Data\n\ntrain_df = retro_dataset(ward)\n\n\ntrain_df.shape\n\n\ntrain_df.head()"
  },
  {
    "objectID": "hylode/HyCastle-Lens.html#live-data-for-prediction",
    "href": "hylode/HyCastle-Lens.html#live-data-for-prediction",
    "title": "HySchool",
    "section": "Live Data for Prediction",
    "text": "Live Data for Prediction\n\npredict_df = live_dataset(ward)\n\n\npredict_df.shape\n\n\npredict_df.head()"
  },
  {
    "objectID": "hylode/HyCastle-Lens.html#example-los-model-lens",
    "href": "hylode/HyCastle-Lens.html#example-los-model-lens",
    "title": "HySchool",
    "section": "Example LoS Model Lens",
    "text": "Example LoS Model Lens\n\nlens = ICUSitRepLoSLens()\n\n\nFit on Training Data\nFocus the Lens on the training dataset\n\nprocessed_train_df = lens.fit_transform(train_df)\n\n\nprocessed_train_df.shape\n\n\nprocessed_train_df.head()\n\n\nprocessed_train_df.dtypes\n\n\nprocessed_train_df.isnull().any()\n\n\n\nTransform Prediction Data\nUse the Lens that was fitted on the training dataset to transform the prediction dataset\n\npredict_df.head()\n\n\nprocessed_predict_df = lens.transform(predict_df)\n\n\nprocessed_predict_df.head()\n\n\nprocessed_predict_df.dtypes\n\n\nprocessed_predict_df.isnull().any()\n\nLive prediction dataset doesn’t have discharge_dts so total_los_td & remaining_los_td are unavailable"
  },
  {
    "objectID": "hylode/HyCastle-Lens.html#sitrep-ui-lens",
    "href": "hylode/HyCastle-Lens.html#sitrep-ui-lens",
    "title": "HySchool",
    "section": "SitRep UI Lens",
    "text": "SitRep UI Lens\n\nlens = ICUSitRepUiLens()\n\n\noutput_df = lens.fit_transform(predict_df)\n\n\noutput_df.head()\n\n\noutput_df.dtypes"
  },
  {
    "objectID": "hylode/HyCastle-Lens.html#crafting-a-new-lens",
    "href": "hylode/HyCastle-Lens.html#crafting-a-new-lens",
    "title": "HySchool",
    "section": "Crafting a new Lens",
    "text": "Crafting a new Lens\n\nRequired imports\n\nfrom typing import *\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import make_column_selector\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline as SKPipeline\nfrom sklearn.impute import MissingIndicator, SimpleImputer\n\nfrom hycastle.icu_store import SITREP_FEATURES\nfrom hycastle.lens.base import BaseLens\nfrom hycastle.lens.icu import *\n\n\n\nAvailable features\nThe following features are available in the ICU SitRep Pipeline\n\nSITREP_FEATURES\n\n\n\nDefinintion\n\nclass CustomICUSitRepLens(BaseLens):\n    \"\"\"\n    Lens to focus ICU SitRep data \n    \"\"\"\n    \n    # Whether to convert all columns in the output dataframe to floating point\n    numeric_output = True\n    \n    # Select a subset of SITREP_COLUMNS to include in this Lens\n    @property\n    def input_cols(self) -> List[str]:\n        return [\n            'episode_slice_id',\n            'episode_key',\n            'admission_dt',\n            'bay_type',\n            'sex',\n            'ethnicity',\n            'admission_age_years',\n            'avg_heart_rate_1_24h',\n            'discharge_ready_1_4h',\n            'n_inotropes_1_4h',\n            'vent_type_1_4h',\n            'wim_1',\n            'elapsed_los_td',\n            'horizon_dt'\n        ]\n\n    specification = ColumnTransformer(\n        [\n            (\n                # Subset of columns that require no pre-processing and can be passed through as is\n                'select',\n                'passthrough',\n                [\n                    'episode_slice_id',\n                    'admission_age_years',\n                    'n_inotropes_1_4h',\n                    'wim_1',\n                ]\n            ),\n            # Pre-processing operations to apply to columns\n            # Input is 3-element tuple:\n            #   - name for pre-processing step\n            #   - transformer instance\n            #   - list of columns to apply to\n            # Any Scikit-Learn Transformer type is valid.\n            # Scikit-Learn Pipeline can also be used to combine multiple transformation for a single column, applied sequentially.\n            # The only caveat is that when a SK Pipeline is used, any transformation that creates new columns, e.g. adding a missing indicator column or\n            # one-hot-encoding, it must be the last step in the Pipeline.\n            # Details & examples for implementing custom pre-processing transformations are in `hycastle.lens.utils` \n            (\n                'admission_dt_exp',\n                DateTimeExploder(),\n                ['admission_dt']\n            ),\n            (\n                'bay_type_enc',\n                OneHotEncoder(),\n                ['bay_type']\n            ),\n            (\n                'sex_enc',\n                OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n                ['sex']\n            ),\n            (\n                'ethnicity_miss',\n                MissingIndicator(\n                    features='all',\n                    missing_values=None\n                ),\n                ['ethnicity']\n            ),\n            (\n                'ethnicity_enc',\n                OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n                ['ethnicity']\n            ),\n            (\n                'discharge_ready_1_4h_enc',\n                OneHotEncoder(handle_unknown='ignore'),\n                ['discharge_ready_1_4h']\n            ),\n            (\n                'vent_type_1_4h_enc',\n                OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n                ['vent_type_1_4h']\n            ),\n            (\n                'avg_heart_rate_1_24h_prep',\n                SKPipeline(\n                    steps=[\n\n                        (\n                            'avg_heart_rate_1_24h_scale',\n                            StandardScaler(),\n                        ),\n                        (\n                            'avg_heart_rate_1_24h_impute',\n                            SimpleImputer(strategy='mean', add_indicator=True),\n                        ),\n                    ]\n                ),\n                ['avg_heart_rate_1_24h']\n            ),\n            (\n                'elapsed_los_td_seconds',\n                FunctionTransformer(timedelta_as_hours),\n                ['elapsed_los_td']\n            ),\n            (\n                'horizon_dt_exp',\n                DateTimeExploder(),\n                ['horizon_dt']\n            ),\n        ]\n    )\n\n\nApplication\n\nlens = CustomICUSitRepLens()\n\n\ntdf = lens.fit_transform(train_df)\n\n\ntdf\n\n\npdf = lens.transform(predict_df)\n\n\npdf"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html",
    "href": "hylode/HyMind-Lab-example.html",
    "title": "HySchool",
    "section": "",
    "text": "Please make a copy of this notebook and do not edit in place\nSee the system diagram for an overview of the HYLODE system components referenced in this notebook.\n(You will need to be signed into GitHub to view)\n\n\n\n\n\nimport pkg_resources\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([f'{i.key}=={i.version}' for i in installed_packages])\n\n\n# Uncomment to list installed packages\n# installed_packages_list\n\n\n\n\nFor a quick installation, uncomment & run the command below (replace ujson with the package you want)\n\n# !pip install ujson\n\n\n# import ujson\n# ujson.__version__\n\nPackages installed this way will disappear when the container is restarted.\nTo have the package permanently available, please log a ticket on ZenHub requesting the package to be added to the HyMind Lab.\n\n\n\n\nfrom datetime import datetime, timedelta\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\nimport urllib\n\nimport arrow\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom hylib.dt import LONDON_TZ, convert_dt_columns_to_london_tz\n\n\n\n\n\nward = 'T03'"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#emap-db",
    "href": "hylode/HyMind-Lab-example.html#emap-db",
    "title": "HySchool",
    "section": "EMAP DB",
    "text": "EMAP DB\nAccess to EMAP is required for multiple components of the system to function properly.\nThis includes some of the functions that are useful to run in the HyMind Lab\n\nEMAP credentials\nEMAP credentials are allocated per user and not stored in the environment variables. You do not want your credentials to leak into the source repository.\nOne way of safeguarding is to create a file called secret at the top level of the HyMind repository (next to this notebook).\nDo this here in Jupyter and not a local copy of the repo.\nThe first line should be your UDS username and the second line should be your UDS password.\nsecret has been added to .gitignore and will be excluded from the repository.\nRead your username & password into the environment:\n\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path('secret').read_text().strip().split('\\n')\n\n\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nCreate a SQLAlchemy Engine for accessing the UDS:\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#hylode-db",
    "href": "hylode/HyMind-Lab-example.html#hylode-db",
    "title": "HySchool",
    "section": "HYLODE DB",
    "text": "HYLODE DB\nThe hylode database is a containerised instance of Postgres 12 used by the various components to store data for use further down the pipeline.\nYou can think of it as the medium of data flow for our system.\nUnlike the uds, it is private to us.\nYou don’t need individual credentials, everthing is baked into the environment variables.\nThere are several schemas, roughly one for each subsystem (see link to system diagram above).\nStoring data in and retrieving data from the hylode database happens through the APIs provided by the hyflow, hygear & hycastle modules.\nDirect interaction with the database is not an expected part of the HyMind workflow and presented here for interest only.\n\ndb_host = os.getenv('HYLODE_DB_HOST')\ndb_name = os.getenv('HYLODE_DB_NAME')\ndb_user = os.getenv('HYLODE_DB_USER')\ndb_passwd = os.getenv('HYLODE_DB_PASSWORD')\ndb_port = os.getenv('HYLODE_DB_PORT')                                                                                                       \n\n\nhydb_engine = create_engine(f'postgresql://{db_user}:{db_passwd}@{db_host}:{db_port}/{db_name}')\n\n\nHyDef\nThe hydef schema in the hylode database contains static reference data\n\nLocations\n\nbeds_df = pd.read_sql(\n    \"\"\"\n    select \n        bed.id\n        ,bed.code\n        ,bed.source_id\n        ,bay.code\n        ,bay.type\n        ,ward.code\n        ,ward.name\n        ,ward.type\n    from\n        hydef.beds bed\n    inner join hydef.bays bay on bed.bay_id = bay.id\n    inner join hydef.wards ward on ward.code = bay.ward_code\n    order by ward.code, bay.code, bed.code\n    \"\"\",\n    hydb_engine\n)\n\n\nbeds_df\n\n\n\nICU Observation Types Catalogue\nA growing list of observation types that we are interested in for the ICU pipeline\n\nicu_obs_types = pd.read_sql('select * from hydef.icu_observation_types', hydb_engine)\n\n\nicu_obs_types"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#retrospective-data",
    "href": "hylode/HyMind-Lab-example.html#retrospective-data",
    "title": "HySchool",
    "section": "Retrospective Data",
    "text": "Retrospective Data\n\ntraining_df = retro_dataset(ward)\n\n\ntraining_df.shape\n\n\ntraining_df.head()\n\n\ntraining_df.columns\n\n\ntraining_df.episode_slice_id.duplicated().any()\n\n\ntraining_df.isnull().any()"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#live-data",
    "href": "hylode/HyMind-Lab-example.html#live-data",
    "title": "HySchool",
    "section": "Live Data",
    "text": "Live Data\nThese functions return personally identifiable information\n\nHyLode Live Episode Slices\n\nprediction_df = live_dataset(ward)\n\n\nprediction_df.shape\n\n\nprediction_df\n\n\n\nEMAP Live Census Snapshot\n\nemap_df = emap_snapshot(ward)\n\n\nemap_df.head()\n\n\n\nFilter\nLimit episode slices used for prediction to admissions that are in the EMAP census\n\nprediction_df.csn.isin(emap_df.csn)\n\n\nprediction_df = prediction_df[prediction_df.csn.isin(emap_df.csn)]\n\n\nprediction_df"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#minimal-example-using-the-hyflow-package",
    "href": "hylode/HyMind-Lab-example.html#minimal-example-using-the-hyflow-package",
    "title": "HySchool",
    "section": "Minimal example using the HyFlow package",
    "text": "Minimal example using the HyFlow package\n\nFetch ICU Episode Slices from EMAP\n\n# the point-in-time we are interested in:  7am on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 7, 0, 0).astimezone(LONDON_TZ)\n\n\nbeds_df = beds_from_hydef(ward)\n\n\nepisode_slices_df = icu_episode_slices_from_emap(ward, horizon_dt, list(beds_df.hl7_location))\n\nThe HyFlow method adds the episode_key as that is a HYLODE concept and not available in EMAP.\n\nepisode_slices_df\n\n\n# Attach HyDef bed_id to episode slice & drop HL7 location string\nepisode_slices_df = episode_slices_df.join(\n    beds_df.loc[:, ['bed_id', 'hl7_location']].set_index('hl7_location'),\n    on='hl7_location'\n).drop(columns=['hl7_location'])\n\n\nepisode_slices_df\n\n\n\nFetch matching Patients from EMAP for Episode Slices that are in in HyFlow\n\n# the point-in-time we are interested in:  8pm on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 20, 0, 0).astimezone(LONDON_TZ)\n\n\n# get our saved Episode Slices\nepisode_slices_df = icu_episode_slices_from_hyflow(ward, horizon_dt)\n\n\nepisode_slices_df.head()\n\n\npatients_df = icu_patients_from_emap(ward, horizon_dt, list(episode_slices_df.csn))\n\n\npatients_df\n\n\n# Attach HyFlow episode_slice_id to patient\npatients_df = patients_df.join(\n    episode_slices_df.loc[:, ['episode_slice_id', 'csn']].set_index('csn'),\n    on='csn'\n).drop(columns=['csn'])\n\n\npatients_df\n\n\n\nFetch matching Observations of interest in EMAP for Episode Slices that are in HyFlow\n\nlookback_hrs = 24 # size of the trailing window we are interested in\n\n\n# the point-in-time we are interested in:  10am on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 10, 0, 0).astimezone(LONDON_TZ)\n\n\n# get our saved Episode Slices\nepisode_slices_df = icu_episode_slices_from_hyflow(ward, horizon_dt)\n\n\nepisode_slices_df.head()\n\n\n# get our reference list of observation types we care about\nobs_types_df = icu_observation_types_from_hydef()\n\n\nobs_types_df.head()\n\n\nobs_df = icu_observations_from_emap(\n    ward,\n    horizon_dt,\n    list(episode_slices_df.csn),\n    list(obs_types_df.source_id),\n    lookback_hrs\n)\n\n\nobs_df\n\n\n# Attach HyDef observation type id to observation\nobs_df = obs_df.join(\n    obs_types_df.loc[:, ['obs_type_id', 'source_id']].set_index('source_id'),\n    on='source_id'\n)\n\n\n# Attach HyFlow episode_slice_id to observation\nobs_df = obs_df.join(\n    episode_slices_df.loc[:, ['episode_slice_id', 'csn']].set_index('csn'),\n    on='csn'\n).drop(columns=['csn'])\n\n\nobs_df"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#accessing-the-hyflow-schema-directly",
    "href": "hylode/HyMind-Lab-example.html#accessing-the-hyflow-schema-directly",
    "title": "HySchool",
    "section": "Accessing the hyflow schema directly",
    "text": "Accessing the hyflow schema directly\nDirectly querying the hylode database will return personally identifiable information\nLike most tables in the hylode database, the hyflow schema tables are all immutable logs.\nThat means data is only ever appended, never updated in place.\nThis also means, for example, that an individual patient will have many records in the icu_patients_log table,\none for each slice that was taken while their episode was active.\nImportant notes about direct Hylode DB access:\nThe queries provided through the functions in the hyflow & hygear packages take care of removing duplicates.\nIf you access the schemas directly you will need to do that yourself - see the various hyflow__*.sql files in hygear/load/sql for examples of partitioning over episode_slice_id and horizon_dt columns.\nOther conveniences are provided by the packages.\nFor example, the Postgres/SQLAlchemy/Pandas stack does not support storing timedeltas directly (even though it is a supported data type in both Postgres & Pandas, SQLAlchemy is unable to handle it).\nThat means the raw span column in the hyflow.icu_episode_slices_log table is in nanoseconds.\nConverting to a timedelta is done in the packages but you’ll have to do that yourself if you access the raw tables.\n\nICU Episode Slices with Bed Id\n\nsql_episode_slices_df = pd.read_sql(\n    '''\n        select \n            ep.id AS episode_slice_id\n            , ep.episode_key\n            , ep.csn\n            , ep.admission_dt\n            , ep.discharge_dt\n            , beds.source_id AS bed\n            , horizon_dt\n            , log_dt\n        from hyflow.icu_episode_slices_log ep\n            inner join hydef.beds beds ON ep.bed_id = beds.id\n            inner join hydef.bays bays ON beds.bay_id = bays.id\n         WHERE bays.ward_code = %(ward)s\n        order by episode_key, horizon_dt limit 1000\n    ''', \n    hydb_engine,\n    params={'horizon_dt': horizon_dt, 'ward': 'T03'}\n)\n\n\nsql_episode_slices_df\n\n\n\nICU Patients\n\nsql_patients_df = pd.read_sql('''\n    select \n        id AS patient_log_id\n        , episode_slice_id\n        , mrn\n        , name\n        , dob\n        , sex\n        , ethnicity\n        , postcode\n        , horizon_dt\n        , log_dt\n    from hyflow.icu_patients_log \n    order by mrn, horizon_dt limit 1000''', \n    hydb_engine\n)\n\n\nsql_patients_df\n\n\n\nICU Observations\n\nsql_obs_df = pd.read_sql('select * from hyflow.icu_observations_log order by episode_slice_id, horizon_dt limit 1000', hydb_engine)\n\n\nsql_obs_df"
  },
  {
    "objectID": "hylode/HyMind-Lab-example.html#single-horizon-example",
    "href": "hylode/HyMind-Lab-example.html#single-horizon-example",
    "title": "HySchool",
    "section": "Single Horizon Example",
    "text": "Single Horizon Example\n\n# the point-in-time we are interested in:  10pm on 31/08/2021 BST\nhorizon_dt = datetime(2021, 8, 31, 22, 0, 0).astimezone(LONDON_TZ)\n\n\nFetch ICU Episode Slices active at a specific point-in-time\n\nepisode_slices_df = icu_episode_slices_from_hyflow(\n    ward,\n    horizon_dt\n)\n\n\nepisode_slices_df\n\n\n\nFetch matching Patients for ICU Episode Slices active at a specific point-in-time\n\n# fetch matching patients\npatients_df = icu_patients_from_hyflow(\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\npatients_df.head()\n\n\n# join episode slices with patients\ncombined_df = episode_slices_df.join(\n        patients_df.loc[:, ['episode_slice_id', 'mrn', 'dob', 'sex', 'ethnicity']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    ).drop(['log_dt', 'horizon_dt'], axis=1)\n\n\ncombined_df.head()\n\n\n\nFetch matching Observations for ICU Episode Slices active at a specific point-in-time\nthis is in long format, multiple rows per episode_slice_id\n\n# number of trailing hours we are interested in\nlookback_hrs = 24\n\n\n# fetch matching observations\nobs_df = icu_observations_from_hyflow(\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id),\n    lookback_hrs\n)\n\n\nobs_df.head()\n\n\n# fetch the observation types reference catalogue\nobs_types_df = icu_observation_types_from_hydef()\n\n\n# join observations with metadata\nobs_df = obs_df.join(\n    obs_types_df.set_index('obs_type_id'),\n    on='obs_type_id'\n)\n\n\nobs_df.head()\n\n\n# join observations with episode slices to get episode key\neps_obs_df = obs_df.join(\n    episode_slices_df.loc[:, ['episode_slice_id', 'episode_key', 'admission_dt', 'discharge_dt']].set_index('episode_slice_id'),\n    on='episode_slice_id'\n)\n\n\neps_obs_df.groupby('episode_key')['obs_id'].count().rename('n_observations')\n\n\n\nFetch generated ICU Patient State Features for ICU Episode Slices active at a specific point-in-time\n\npatient_state_df = icu_features_from_hygear(\n    'patient_state',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\npatient_state_df\n\n\n# join with patient state\ncombined_df = combined_df.join(\n        patient_state_df.loc[:, ['episode_slice_id', 'is_proned_1_4h', 'discharge_ready_1_4h', 'is_agitated_1_8h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Therapeutics Features for ICU Episode Slices active at a specific point-in-time\n\ntherapeutics_df = icu_features_from_hygear(\n    'therapeutics',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\ntherapeutics_df\n\n\n# join with therapeutics\ncombined_df = combined_df.join(\n        therapeutics_df.loc[:, ['episode_slice_id', 'n_inotropes_1_4h', 'had_nitric_1_8h', 'had_rrt_1_4h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Ventilation Features for ICU Episode Slices active at a specific point-in-time\n\nventilation_df = icu_features_from_hygear(\n    'ventilation',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\nventilation_df\n\n\n# join with ventilation\ncombined_df = combined_df.join(\n        ventilation_df.loc[:, ['episode_slice_id', 'had_trache_1_12h', 'vent_type_1_4h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Vitals Features for ICU Episode Slices active at a specific point-in-time\n\nvitals_df = icu_features_from_hygear(\n    'vitals',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\nvitals_df\n\n\n# join with vitals\ncombined_df = combined_df.join(\n        vitals_df.loc[:, ['episode_slice_id', 'avg_heart_rate_1_24h', 'max_temp_1_12h', 'avg_resp_rate_1_24h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Work Intensity Metric Features for ICU Episode Slices active at a specific point-in-time\n\nwim_df = icu_features_from_hygear(\n    'work_intensity',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\nwim_df\n\n\n# join with work intensity\ncombined_df = combined_df.join(\n        wim_df.loc[:, ['episode_slice_id', 'wim_1']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Temporal Features for ICU Episode Slices active at a specific point-in-time\n\ntemporal_df = icu_features_from_hygear(\n    'temporal',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\ntemporal_df\n\n\n# join with temporal\ncombined_df = combined_df.join(\n        temporal_df.loc[:, ['episode_slice_id', 'elapsed_los_td', 'total_los_td', 'remaining_los_td',]].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nCombined\n\ncombined_df.shape\n\n\ncombined_df.head()"
  },
  {
    "objectID": "hylode/hylode.html",
    "href": "hylode/hylode.html",
    "title": "HySchool",
    "section": "",
    "text": "Documentation specific to the HYLODE project"
  },
  {
    "objectID": "faq/emap_vital_signs.html",
    "href": "faq/emap_vital_signs.html",
    "title": "Vital signs in EMAP",
    "section": "",
    "text": "Answer\nThe following script should return any vital signs recorded in the last 5 minutes!\nSELECT\n  -- observation details\n   ob.visit_observation_id\n  ,ob.hospital_visit_id\n  ,ob.observation_datetime\n\n  --,ob.visit_observation_type_id\n  --,ot.id_in_application\n\n  -- label nicely\n  ,CASE\n    WHEN ot.id_in_application = '10' THEN 'SpO2'\n    WHEN ot.id_in_application = '5' THEN 'BP'\n    WHEN ot.id_in_application = '3040109304' THEN 'Oxygen'\n    WHEN ot.id_in_application = '6' THEN 'Temp'\n    WHEN ot.id_in_application = '8' THEN 'Pulse'\n    WHEN ot.id_in_application = '9' THEN 'Resp'\n    WHEN ot.id_in_application = '6466' THEN 'AVPU'\n\n  END AS vital\n\n  ,ob.value_as_real\n  ,ob.value_as_text\n  ,ob.unit\n\nFROM\n  star.visit_observation ob\n-- observation look-up\nLEFT JOIN\n  star.visit_observation_type ot\n  on ob.visit_observation_type_id = ot.visit_observation_type_id\n\nWHERE\nob.observation_datetime > NOW() - '5 MINS'::INTERVAL\nAND\not.id_in_application in\n\n  (\n  '10'            --'SpO2'\n  ,'5'            --'BP'\n  ,'3040109304'   --'Room Air or Oxygen'\n  ,'6'            --'Temp'\n  ,'8'            --'Pulse'\n  ,'9'            --'Resp'\n  ,'6466'         -- Level of consciousness\n)\nORDER BY ob.observation_datetime DESC\n;"
  },
  {
    "objectID": "faq/faq.html",
    "href": "faq/faq.html",
    "title": "FAQ, How do I?, TIL…",
    "section": "",
    "text": "Try to keep each post short.\nWe recommend\n\nthe title should be a question\nthe content is a direct answer to that question\n\nIf it get’s long then consider upgrading the post to a tutorial.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nInvalid Date\n\n\nHow do I learn a good programming style?\n\n\nSteve Harris\n\n\n\n\nJul 5, 2022\n\n\nHow to use the Python debugger inside docker\n\n\nSteve Harris\n\n\n\n\nSep 3, 2022\n\n\nVital signs in EMAP\n\n\nSteve Harris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "faq/python-debug-in-docker.html",
    "href": "faq/python-debug-in-docker.html",
    "title": "How to use the Python debugger inside docker",
    "section": "",
    "text": "Answer\nI got massively stuck trying to debug a python script in a running docker container. On my own machine, I would have just used the Python debugger. This guide shows how to make that work from a running docker container."
  },
  {
    "objectID": "faq/how-to-learn-to-program.html",
    "href": "faq/how-to-learn-to-program.html",
    "title": "How do I learn a good programming style?",
    "section": "",
    "text": "Answer\nA crowd-sourced list of recommendations for learning good programming style from the lab group.\n\nSoftware Design in Python\nPragmatic Programmer\nClean Code\nPhilosophy of Software Design\nCode Craft\nThe Missing README\nBeyond the Basic Stuff\nRefactoring code\nAll the little things\nNothing is something"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About",
    "section": "",
    "text": "Steve Harris\n\n\n\n\n\n\nSteve Harris\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/steve-harris.html",
    "href": "about/steve-harris.html",
    "title": "Steve Harris",
    "section": "",
    "text": "Hello. This is all about me!"
  },
  {
    "objectID": "appendix/safe-data-python.html",
    "href": "appendix/safe-data-python.html",
    "title": "HySchool",
    "section": "",
    "text": "# To run videos from Youtube etc\nfrom IPython.display import HTML, Image\nAcknowledgement to Safe Data Access Professionals Working Group\nPlease review SDC handbook"
  },
  {
    "objectID": "appendix/safe-data-python.html#five-safes",
    "href": "appendix/safe-data-python.html#five-safes",
    "title": "HySchool",
    "section": "Five Safes",
    "text": "Five Safes\nWe follow the ‘Five Safes’ approach to managing data and information security. This means that we don’t rely on just the ‘safety’ of the data but also take into account the following:\n\nSafe People\n\nall individuals have substantive contracts or educational relationships with higher education or NHS institutions\nthose working need to have evidence of experience of working with such data (e.g. previous training, previous work with ONS, data safe havens etc.) or they need a supervisor who can has similar experience\nthose working need to undergo training in information governance and issues with statistical disclosure control (SDC)\n\n\n\nSafe Projects\n\nprojects must ‘serve the public good’ \nprojects must meet relevant HRA and UCLH research and ethics approvals\nservice delivery work mandated as per usual trust processes\n\n\n\nSafe Settings\n\nworking at UCLH in the NHS on approved infrastructure\nUCLH local and remote desktops\nUCLH Data Science Desktop\nGeneric Application Development Environment (GADE)\n\n\n\nSafe Outputs\n\noutputs (e.g. reports, figures and tables) must be non-disclosing \noutputs should remain on NHS systems initially\na copy of all outputs that are released externally (documents) should be stored in one central location so that there is visibility for all\n\n\n\nSafe Data\n\ndirect identifiers (hospital numbers, NHS numbers, names etc) should be masked unless there is an explicit justification for their use\ndata releases are proportionate (e.g. limited by calendar periods, by patient cohort etc.)\nfurther work to obscure or mask the data is not necessary given the other safe guards (as per the recommendation by the UK data service)\n\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mln9T52mwj0?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
  },
  {
    "objectID": "appendix/safe-data-python.html#five-safes-at-uclh",
    "href": "appendix/safe-data-python.html#five-safes-at-uclh",
    "title": "HySchool",
    "section": "Five safes at UCLH",
    "text": "Five safes at UCLH\nPractically although this means that we are judging your data safety on more than just the qualities of the data, we are able to work with data that would otherwise be considered unsafe. The plot below demonstrates this by comparing the effort we would have to expend on safety if we wanted to release data on the internet. This means that we lose all the other 4 safes."
  },
  {
    "objectID": "appendix/safe-data-python.html#anonymisation-is-really-hard",
    "href": "appendix/safe-data-python.html#anonymisation-is-really-hard",
    "title": "HySchool",
    "section": "Anonymisation (is really hard)",
    "text": "Anonymisation (is really hard)\nMethods include Generalised Adversarial Networks, differentially-private Bayesian generative models, and Statistical Disclosure Control\n\nStatistical Disclosure Control\nSet thresholds for\n\nk-anonymity: counts the number of individuals identified by the intersection of key variables\nl-diversity: counts how varied other sensitive fields are within a k-anonymous group\n\nThen define\n\ndirect identifiers\nkey variables (indirect identifiers)\nsensitive fields\nnon-identifying variables"
  },
  {
    "objectID": "appendix/safe-data-python.html#frequency-tables",
    "href": "appendix/safe-data-python.html#frequency-tables",
    "title": "HySchool",
    "section": "Frequency tables",
    "text": "Frequency tables\nRules-based - Minimum cell count - All counts should be unweighted\nPrinciples-based - Threshold is a ‘rule-of-thumb’ - The units and data being presented should be considered\nFrequencies can be presented in many different ways including tables, histograms, pie charts, bar charts. The guidance for frequency tables will also apply for these."
  },
  {
    "objectID": "appendix/safe-data-python.html#graphs-and-figures",
    "href": "appendix/safe-data-python.html#graphs-and-figures",
    "title": "HySchool",
    "section": "Graphs and Figures",
    "text": "Graphs and Figures\nExample issues include\n\nhistograms: often low counts in the tails of the distribution, the maximum and minimum values may also be shown.\nscatter plots: by definition are plots of individuals (also residual plots); consider grouping"
  },
  {
    "objectID": "appendix/safe-data-python.html#four-eyes-principle",
    "href": "appendix/safe-data-python.html#four-eyes-principle",
    "title": "HySchool",
    "section": "Four eyes principle",
    "text": "Four eyes principle"
  },
  {
    "objectID": "appendix/safe-data-python.html#gitignore-is-your-friend",
    "href": "appendix/safe-data-python.html#gitignore-is-your-friend",
    "title": "HySchool",
    "section": ".gitignore is your friend",
    "text": ".gitignore is your friend\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# secret\nsecret*\n.env\n\n# scratches\nscratch.ipynb\nUntitled.ipynb\nlabbooks/*\ntmp/*"
  },
  {
    "objectID": "appendix/safe-data-python.html#do-not-hard-code-usernames-and-passwords",
    "href": "appendix/safe-data-python.html#do-not-hard-code-usernames-and-passwords",
    "title": "HySchool",
    "section": "Do not ‘hard code’ usernames and passwords",
    "text": "Do not ‘hard code’ usernames and passwords\nUse a .env file (or secrets or similar) Exclude from git via .gitignore\n# edit this file and replace with actual usernames and passwords\n# then save WITH the dot prefix e.g. env --> .env\n\n# DO NOT SAVE as env (without the dot prefix) \n# else you will publish your secrets to github\n\n# environment variables\nEMAP_DB_USER=YOUR_USERNAME_HERE\nEMAP_DB_PASSWORD=YOUR_PASSWORD_HERE\n\nLoad your environment variables in Python\n\nfrom dotenv import load_dotenv\nload_dotenv('.env')\n\nA template JupyterNotebook for working with data at UCLH. The following features of this notebook, and associated files are documented here to minimise the risk of data leaks or other incidents.\n\nUsernames and passwords are stored in a .env file that is excluded from version control. The example env file at ./config/env should be edited and saved as ./config/.env. A utility function load_env_vars() is provided that will confirm this file exists and load the configuration into the working environment.\n.gitattributes are set to strip JupyterNotebook cells when pushing to GitHub\n\n\nimport os\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\ndef load_env_vars(\n    ENV_FILE_ID = 'rainy.fever.song',\n    dotenv_path = './config/.env'\n                 ):\n    \"\"\"\n    Load environment variables or raise error if the file is not found\n    \"\"\"\n    dotenv_path = Path(dotenv_path)\n    load_dotenv(dotenv_path=dotenv_path)\n\n    if os.getenv('ENV_FILE_ID') != ENV_FILE_ID:\n        raise FileNotFoundError(\"\"\"\n        IMPORTANT\n        An environment file holding the ENV_FILE_ID variable equal to 'rainy.fever.song'\n        should have been found at the ./config/.env path.\n\n        Is the script being run from the repository root (emap-helper/)?\n        Did you convert the example 'env' file to the '.env' file?\n\n        Please check the above and try again \n        \"\"\")\n    else:\n        return True\n\n\nfrom sqlalchemy import create_engine\n\ndef make_emap_engine(db):\n    # Load environment variables\n    load_env_vars()\n    \n    if db == 'uds':\n        # expects to run in HYLODE so these are part of this env\n        host = os.getenv('EMAP_DB_HOST')\n        name = os.getenv('EMAP_DB_NAME')\n        port = os.getenv('EMAP_DB_PORT')\n        user = os.getenv('EMAP_DB_USER')\n        passwd = os.getenv('EMAP_DB_PASSWORD')\n    elif db == 'ids':\n        host = os.getenv('IDS_DB_HOST')\n        name = os.getenv('IDS_DB_NAME')\n        port = os.getenv('IDS_DB_PORT')\n        user = os.getenv('IDS_DB_USER')\n        passwd = os.getenv('IDS_DB_PASSWORD')\n    else:\n        raise ValueError(\"db is not recognised; should be one of 'uds' or 'ids'\")\n        \n    # Construct the PostgreSQL connection\n    emapdb_engine = create_engine(f'postgresql://{user}:{passwd}@{host}:{port}/{name}')\n    return emapdb_engine"
  },
  {
    "objectID": "appendix/safe-data-python.html#do-not-leak-from-jupyter-notebooks",
    "href": "appendix/safe-data-python.html#do-not-leak-from-jupyter-notebooks",
    "title": "HySchool",
    "section": "Do not leak from Jupyter Notebooks",
    "text": "Do not leak from Jupyter Notebooks"
  },
  {
    "objectID": "appendix/safe-data-python.html#gitattributes-to-strip-outputs-from-cells-in-a-notebook",
    "href": "appendix/safe-data-python.html#gitattributes-to-strip-outputs-from-cells-in-a-notebook",
    "title": "HySchool",
    "section": ".gitattributes to strip outputs from cells in a notebook",
    "text": ".gitattributes to strip outputs from cells in a notebook\n*.ipynb filter=strip-notebook-output"
  },
  {
    "objectID": "appendix/safe-data-python.html#for-r",
    "href": "appendix/safe-data-python.html#for-r",
    "title": "HySchool",
    "section": "For R …",
    "text": "For R …\nUse .Renviron as you would .env but it is automatically ‘read’ by R when it starts\n# IMPORTANT\n# DO NOT ADD THE .Renviron VERSION OF THIS FILE TO VERSION CONTROL\n# RENAME from dotrenviron to .Renviron in place and then update the\n# environment variables with actual values\n\n# IDS access\nIDS_PWD=foo\nIDS_HOST=bar\nIDS_USER=me\n\n# UDS access\nUDS_PWD=foo\nUDS_HOST=bar\nUDS_USER=me\n\n# Internet access\nhttp_proxy=http://my-hospital.nhs.uk:1234/\nHTTP_PROXY=http://my-hospital.nhs.uk:1234/\nhttps_proxy=http://my-hospital.nhs.uk:1234/\nHTTPS_PROXY=http://my-hospital.nhs.uk:1234/\n\n\n# https://rstudio.github.io/renv/articles/docker.html\nRENV_PATHS_CACHE=/home/rstudio/renv"
  },
  {
    "objectID": "appendix/safe-health-data.html",
    "href": "appendix/safe-health-data.html",
    "title": "Safe data",
    "section": "",
    "text": "We follow the ‘Five Safes’ approach to managing data and information security. This means that we don’t rely on just the ‘safety’ of the data but also take into account the following:\n\n\n\nall individuals have substantive contracts or educational relationships with higher education or NHS institutions\nthose working need to have evidence of experience of working with such data (e.g. previous training, previous work with ONS, data safe havens etc.) or they need a supervisor who can has similar experience\nthose working need to undergo training in information governance and issues with statistical disclosure control (SDC)\n\n\n\n\n\nprojects must ‘serve the public good’\nprojects must meet relevant HRA and UCLH research and ethics approvals\nservice delivery work mandated as per usual trust processes\n\n\n\n\n\nworking at UCLH in the NHS on approved infrastructure\nUCLH local and remote desktops\nUCLH Data Science Desktop\nGeneric Application Development Environment (GADE)\n\n\n\n\n\noutputs (e.g. reports, figures and tables) must be non-disclosing\noutputs should remain on NHS systems initially\na copy of all outputs that are released externally (documents) should be stored in one central location so that there is visibility for all\n\n\n\n\n\ndirect identifiers (hospital numbers, NHS numbers, names etc) should be masked unless there is an explicit justification for their use\ndata releases are proportionate (e.g. limited by calendar periods, by patient cohort etc.)\nfurther work to obscure or mask the data is not necessary given the other safe guards (as per the recommendation by the UK data service)\n\n\n\n\n\nThe majority of this content is derived and adapted from the Safe Data Access Professional’s Working group, and we strongly recommend reviewing their handbook]\n\n\n\n\n\nPractically although this means that we are judging your data safety on more than just the qualities of the data, we are able to work with data that would otherwise be considered unsafe. The plot below demonstrates this by comparing the effort we would have to expend on safety if we wanted to release data on the internet. This means that we lose all the other 4 safes.\n\n\n\nSafety through disclosure control alone is much harder than using the other 4 ‘safes’"
  },
  {
    "objectID": "appendix/safe-health-data.html#anonymisation-is-really-hard",
    "href": "appendix/safe-health-data.html#anonymisation-is-really-hard",
    "title": "Safe data",
    "section": "Anonymisation (is really hard)",
    "text": "Anonymisation (is really hard)\nMethods include Generalised Adversarial Networks, differentially-private Bayesian generative models, and Statistical Disclosure Control\n\nStatistical Disclosure Control\nSet thresholds for\n\nk-anonymity: counts the number of individuals identified by the intersection of key variables\nl-diversity: counts how varied other sensitive fields are within a k-anonymous group\n\nThen define\n\ndirect identifiers\nkey variables (indirect identifiers)\nsensitive fields\nnon-identifying variables\n\n\n\n\nFigure 1: Initial procedural steps to reduce re-identification risk and specifying a priori the statistical disclosure control thresholds.\n\n\n\n\n\nFigure 2: Algorithim that iteratively examines the disclosure risk and applies noise or aggregates data until this meets the thresholds specified above\n\n\n\n\n\n\n\n\nNote\n\n\n\nTypes of disclosure\n\nPrimary disclosure\n\nInferring the identity, and/or information about, a data subject from a single source of data.\n\nSecondary disclosure (‘attribute’)\n\nDeriving the identity, and/or information of, a data subjecting by combining two or more sources of information together."
  },
  {
    "objectID": "appendix/safe-health-data.html#frequency-tables",
    "href": "appendix/safe-health-data.html#frequency-tables",
    "title": "Safe data",
    "section": "Frequency tables",
    "text": "Frequency tables\nRules-based - Minimum cell count - All counts should be unweighted\nPrinciples-based - Threshold is a ‘rule-of-thumb’ - The units and data being presented should be considered\nFrequencies can be presented in many different ways including tables, histograms, pie charts, bar charts. The guidance for frequency tables will also apply for these."
  },
  {
    "objectID": "appendix/safe-health-data.html#graphs-and-figures",
    "href": "appendix/safe-health-data.html#graphs-and-figures",
    "title": "Safe data",
    "section": "Graphs and Figures",
    "text": "Graphs and Figures\nExample issues include\n\nhistograms: often low counts in the tails of the distribution, the maximum and minimum values may also be shown.\nscatter plots: by definition are plots of individuals (also residual plots); consider grouping"
  },
  {
    "objectID": "appendix/safe-health-data.html#four-eyes-principle",
    "href": "appendix/safe-health-data.html#four-eyes-principle",
    "title": "Safe data",
    "section": "Four eyes principle",
    "text": "Four eyes principle"
  }
]